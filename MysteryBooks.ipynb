{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f338314",
   "metadata": {},
   "source": [
    "## LXML SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4eebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html as web\n",
    "from lxml.etree import XPath\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4885e",
   "metadata": {},
   "source": [
    "### Making URLs and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9fd2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = \"http://books.toscrape.com/\"\n",
    "bookUrl = baseUrl + \"catalogue/category/books/mystery_3/index.html\"\n",
    "pageUrl = baseUrl + \"catalogue/category/books/mystery_3/page-\"  # page-1,page-2 found\n",
    "columns = [\"title\", \"price\", \"stock\", \"imageUrl\", \"rating\", \"url\"]  # for CSV header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd698a",
   "metadata": {},
   "source": [
    "### Making empty Dataset and default page values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a5b8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = []\n",
    "page=1\n",
    "totalPages = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f2471",
   "metadata": {},
   "source": [
    "### Now save dataset to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64323ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeto_csv(data, filename, columns):\n",
    "    with open(filename, \"w+\", newline=\"\", encoding=\"UTF-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for element in data:\n",
    "            writer.writerows([element])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fa9e7",
   "metadata": {},
   "source": [
    "### Loop through pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e941f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalPages found: 2\n",
      "Processing Page 1 from  2\n",
      "Rows in Dataset:  20\n",
      "Processing Page 2 from  2\n",
      "Rows in Dataset:  32\n"
     ]
    }
   ],
   "source": [
    "while page <= totalPages:\n",
    "    source = web.parse(pageUrl + str(page) + \".html\").getroot()  # read and parse\n",
    "    if page == 1:  # pagination\n",
    "        perpageArticles = source.xpath(\n",
    "            '//form[@class=\"form-horizontal\"]/strong[3]/text()'\n",
    "        )  # 20\n",
    "        totalArticles = source.xpath(\n",
    "            '//form[@class=\"form-horizontal\"]/strong[1]/text()'\n",
    "        )  # 29\n",
    "        totalPages = math.ceil(\n",
    "            int(totalArticles[0]) / int(perpageArticles[0])\n",
    "        )  # 1.45 ceil up\n",
    "        print(\"TotalPages found:\", totalPages)\n",
    "    print(\"Processing Page \" + str(page) + \" from \", totalPages)\n",
    "\n",
    "    # individual path for chosen elements\n",
    "    articles = XPath(\"//ol[contains(@class,'row')]/li[position()>0]\")  # block\n",
    "    titlePath = XPath(\".//article[contains(@class,'product_pod')]/h3/a/@title\")\n",
    "    linkPath = XPath(\".//article[contains(@class,'product_pod')]/h3/a/@href\")\n",
    "    pricePath = XPath(\".//article/div[2]/p[contains(@class,'price_color')]/text()\")\n",
    "    stockPath = XPath(\n",
    "        \".//article/div[2]/p[contains(@class,'availability')]/text()[normalize-space()]\"\n",
    "    )\n",
    "    imagePath = XPath(\n",
    "        \".//article/div[1][contains(@class,'image_container')]/a/img/@src\"\n",
    "    )\n",
    "    ratingPath = XPath(\".//article/p[contains(@class,'star-rating')]/@class\")\n",
    "\n",
    "    # iterate through all articles and individual element path\n",
    "    for row in articles(source):\n",
    "        title = titlePath(row)[0].strip()\n",
    "        link = linkPath(row)[0].replace(\"../../../\", baseUrl + \"catalogue/\").strip()\n",
    "        price = pricePath(row)[0]\n",
    "        availability = stockPath(row)[0].strip()\n",
    "        image = imagePath(row)[0].replace(\"../../../../\", baseUrl).strip()\n",
    "        rating = ratingPath(row)[0].replace(\"star-rating\", \"\").strip()\n",
    "\n",
    "        # if title is not missing, add to dataSet\n",
    "        if len(title) > 0:\n",
    "            dataSet.append([title, price, availability, image, rating, link])\n",
    "\n",
    "    print(\"Rows in Dataset: \", len(dataSet))\n",
    "    page += 1  # increment page for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36033aa",
   "metadata": {},
   "source": [
    "### Convert list(dataSet) to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98ec9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeto_csv(dataSet, \"MysteryBooks.csv\", columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_wbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
